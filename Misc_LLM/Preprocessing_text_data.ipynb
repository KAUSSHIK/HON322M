{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eef4ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b141993",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/kausshik/HON322M/Misc_LLM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17b03099",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Hotel_review.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6a0d425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hotel</th>\n",
       "      <th>urls</th>\n",
       "      <th>reviewer</th>\n",
       "      <th>review_overall</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>ReviewerValue</th>\n",
       "      <th>ReviewerLocation</th>\n",
       "      <th>ReviewerService</th>\n",
       "      <th>ReviewerRooms</th>\n",
       "      <th>ReviewerCleanliness</th>\n",
       "      <th>ReviewerSleepQuality</th>\n",
       "      <th>Date of Stay</th>\n",
       "      <th>Trip Type</th>\n",
       "      <th>Contributions</th>\n",
       "      <th>Helpful Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LondonHouse Chicago, Curio Collection by Hilton</td>\n",
       "      <td>https://www.tripadvisor.com/Hotel_Review-g3580...</td>\n",
       "      <td>Jen H</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Manny the Man</td>\n",
       "      <td>The London House Cupola is breathtaking and th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>December 2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LondonHouse Chicago, Curio Collection by Hilton</td>\n",
       "      <td>https://www.tripadvisor.com/Hotel_Review-g3580...</td>\n",
       "      <td>CorMom</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Amazing Views and Location</td>\n",
       "      <td>We stayed here for a family vacation to go to ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>August 2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LondonHouse Chicago, Curio Collection by Hilton</td>\n",
       "      <td>https://www.tripadvisor.com/Hotel_Review-g3580...</td>\n",
       "      <td>K W</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Phenomenal Service</td>\n",
       "      <td>I had a wonderful time at the Londonhouse. So ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>December 2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LondonHouse Chicago, Curio Collection by Hilton</td>\n",
       "      <td>https://www.tripadvisor.com/Hotel_Review-g3580...</td>\n",
       "      <td>Daniel L</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Great hotel in a great location!</td>\n",
       "      <td>We chose this hotel because it offered great v...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>December 2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LondonHouse Chicago, Curio Collection by Hilton</td>\n",
       "      <td>https://www.tripadvisor.com/Hotel_Review-g3580...</td>\n",
       "      <td>Himan</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Great customer service</td>\n",
       "      <td>I had a great time at LondonHouse. The staff, ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>January 2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             hotel  \\\n",
       "0  LondonHouse Chicago, Curio Collection by Hilton   \n",
       "1  LondonHouse Chicago, Curio Collection by Hilton   \n",
       "2  LondonHouse Chicago, Curio Collection by Hilton   \n",
       "3  LondonHouse Chicago, Curio Collection by Hilton   \n",
       "4  LondonHouse Chicago, Curio Collection by Hilton   \n",
       "\n",
       "                                                urls   reviewer  \\\n",
       "0  https://www.tripadvisor.com/Hotel_Review-g3580...     Jen H    \n",
       "1  https://www.tripadvisor.com/Hotel_Review-g3580...    CorMom    \n",
       "2  https://www.tripadvisor.com/Hotel_Review-g3580...       K W    \n",
       "3  https://www.tripadvisor.com/Hotel_Review-g3580...  Daniel L    \n",
       "4  https://www.tripadvisor.com/Hotel_Review-g3580...     Himan    \n",
       "\n",
       "   review_overall                             title  \\\n",
       "0             5.0                     Manny the Man   \n",
       "1             5.0        Amazing Views and Location   \n",
       "2             5.0                Phenomenal Service   \n",
       "3             5.0  Great hotel in a great location!   \n",
       "4             5.0            Great customer service   \n",
       "\n",
       "                                                text  ReviewerValue  \\\n",
       "0  The London House Cupola is breathtaking and th...            NaN   \n",
       "1  We stayed here for a family vacation to go to ...            5.0   \n",
       "2  I had a wonderful time at the Londonhouse. So ...            5.0   \n",
       "3  We chose this hotel because it offered great v...            5.0   \n",
       "4  I had a great time at LondonHouse. The staff, ...            5.0   \n",
       "\n",
       "   ReviewerLocation  ReviewerService  ReviewerRooms  ReviewerCleanliness  \\\n",
       "0               NaN              NaN            NaN                  NaN   \n",
       "1               5.0              5.0            NaN                  NaN   \n",
       "2               NaN              5.0            5.0                  NaN   \n",
       "3               5.0              5.0            5.0                  5.0   \n",
       "4               NaN              5.0            NaN                  NaN   \n",
       "\n",
       "   ReviewerSleepQuality   Date of Stay Trip Type  Contributions  Helpful Vote  \n",
       "0                   NaN  December 2021       NaN              4           1.0  \n",
       "1                   NaN    August 2021       NaN             98          79.0  \n",
       "2                   NaN  December 2021       NaN              1           NaN  \n",
       "3                   5.0  December 2021       NaN             70           6.0  \n",
       "4                   5.0   January 2022       NaN              1           NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255fb04f",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "1. ***Tokenization*** - The process of breaking down the text into individual words or \"tokens\".\n",
    "\n",
    "2. ***Stopwords Removal*** - Eliminating frequently occurring but less meaningful words (e.g., \"the,\" \"and,\" \"is\") from text data\n",
    "\n",
    "3. ***Stemming and Lemmatization*** - reduce words to their base or root form. \n",
    "\n",
    "    - ***Stemming*** - Simplify words by removing suffixes (e.g., running, runs, and run), but it might result in non-words (e.g., \"leaves\" -> \"leav\"). \n",
    "    \n",
    "    - ***Lemmatization*** - on the other hand, transforms words into their original form while retaining their meaning (e.g., \"leaves\" -> \"leaf\").\n",
    "\n",
    "5. ***Vectorization*** - The conversion of text into a numerical format, such as vectors. A common approach is the bag-of-words model, where a matrix is created to store word frequencies (word counts) for each document or text in the corpus. This process is often referred to as the vectorization of the raw text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdbfb1f",
   "metadata": {},
   "source": [
    "### Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56ca3f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7cb2ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2deeaacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if only apply split to segregate words\n",
    "first_text = data['text'][0]\n",
    "\n",
    "print(first_text)\n",
    "print(\"=\"*90)\n",
    "print(first_text.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9016a16",
   "metadata": {},
   "source": [
    "\"comma and period\" are not seperated as one term; e.g., \"proposal.\"\n",
    "\n",
    "nltk library \"word_tokenize()\": split singular words as well as puctuations into separate elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628d2544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using nltk.word_tokenize()\n",
    "first_text_list = nltk.word_tokenize(first_text)\n",
    "\n",
    "print(first_text_list)\n",
    "# now it's well seprated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be7d0be",
   "metadata": {},
   "source": [
    "Another popular pre-processing library for text data is `spacy`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b69d41",
   "metadata": {},
   "source": [
    "### Stopword Removal\n",
    "Stop words include terms such as \"to\" or \"the\" and therefore, it would be to our benefit to remove them during the pre-processing phase.\n",
    "\n",
    "NLTK comes with a list of 179 english stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab60142d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc33cfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using nltk.corpus.stopwords.words('english') to remove stop words\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993babe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ce043e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep words that do not contain stopwords\n",
    "first_text_list_cleaned = [word for word in first_text_list if word.lower() not in stopwords]\n",
    "\n",
    "print(first_text_list_cleaned)\n",
    "print(\"=\"*90)\n",
    "print(\"Length of original list: {0} words\\n\"\n",
    "     \"Length of list after stopwords removal: {1} words\"\n",
    "     .format(len(first_text_list), len(first_text_list_cleaned)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c8d788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as [word for word in first_text_list if word.lower() not in stopwords]:\n",
    "first_text_list_cleaned = []\n",
    "\n",
    "for word in first_text_list:\n",
    "    if word.lower() not in stopwords:\n",
    "        first_text_list_cleaned.append(word)\n",
    "        \n",
    "print(first_text_list_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e62449",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization\n",
    "After removal of stopwords, the next stage of NLP that I would like to introduce is the process of Stemming. The work at this stage attempts to reduce as many different variations of similar words into a single term ( different branches all reduced to single word stem). Therefore if we have \"running\", \"runs\" and \"run\", we would really want these three distinct words to collapse into just the word \"run\". (However of course we lose the past, present or future tense)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c17322",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f73ce8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The stemmed form of running is: {}\".format(stemmer.stem(\"running\")))\n",
    "print(\"The stemmed form of runs is : {}\".format(stemmer.stem(\"runs\")))\n",
    "print(\"The stemmed form of run is : {}\".format(stemmer.stem(\"run\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8416c91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The stemmed form of leaves is: {}\".format(stemmer.stem(\"leaves\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf6afd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a686959",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# define lemm, and use lemm.lemmatize() to do lemmatization\n",
    "lemm = WordNetLemmatizer()\n",
    "print(\"The lemmatized form of leaves is: {}\".format(lemm.lemmatize(\"leaves\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8751fc",
   "metadata": {},
   "source": [
    "The lemmatizer is working; making the words much more lexical sense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8092ef",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "\n",
    "A machine can read in bits and numbers and therefore we will first need to convert our text into numbers (Machine learning algorithms operate on a numeric feature space) for which we utilise a very common approach known as the Bag-of-Words.\n",
    "\n",
    "***The Bag of Words approach*** -\n",
    "This approach uses the counts of words and records the occurrence of each word. For example given these two sentences \"I love to eat Burgers\", \"I love to eat Fries\", we first tokenize to obtain our vocabulary of 6 words from which we can get the word counts for - [I, love, to, eat, Burgers, Fries].\n",
    "\n",
    "Each word is a feature and each row is a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dc4302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer(): Convert a collection of text documents to a matrix of token counts\n",
    "# Defining our sentence\n",
    "sentence = [\"I love to eat Burgers\",\n",
    "           \"I love to eat Fries\"]\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words= \"english\") \n",
    "\n",
    "# fit\n",
    "sentence_transform = vectorizer.fit_transform(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3cac96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The features are: \\n {}\".format(vectorizer.get_feature_names_out())) # \\n: add new line\n",
    "print(\"The vectorized array looks like: \\n {}\".format(sentence_transform.toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21556634",
   "metadata": {},
   "source": [
    "First row: 1 burger, 1 eat, 0 fries, 1 love\n",
    "\n",
    "Second row: 0 burger, 1 eat, 1 fries, 1 love"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce141a5",
   "metadata": {},
   "source": [
    "### Putting all the preprocessing steps together\n",
    "Do not need to go through all the steps in tokenization, stopword removals, stemming/lemmatizing, and vectorization.\n",
    "\n",
    "Sklearn's tokenizer discards all single character terms like ('a', 'w' etc) and also lower cases all terms by default. Filtering out stopwords in Sklearn is as convenient as passing the value 'english' into the argument \"stop_words\" where a built-in English stopword list is automatically used.\n",
    "\n",
    "Unfortunately, there is no built-in lemmatizer in the vectorizer while we can extend the CountVectorizer class by overwriting the \"build_analyzer\" method as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6591fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# define lemm to do lemmatization\n",
    "lemm = WordNetLemmatizer()\n",
    "\n",
    "# define a class to extend the CountVectorizer class with a lemmatizer\n",
    "class LemmaCountVectorizer(CountVectorizer):\n",
    "\n",
    "    # build_analyzer(): Return a callable to process input data. \n",
    "    # The callable handles that handles preprocessing, tokenization, and n-grams generation.\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(LemmaCountVectorizer, self).build_analyzer()   # self=countvectorizer\n",
    "        return lambda doc: (lemm.lemmatize(w) for w in analyzer(doc))\n",
    "        \n",
    "#  super() method lets you access methods in a parent class. You can think of super() as a way to jump up to \n",
    "# view the methods in the class from which another class is inherited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de76474",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"I love to eat Burgers\", \"I love to eat Fries\"]\n",
    "\n",
    "tf_vectorizer = LemmaCountVectorizer(stop_words = \"english\")\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(text)\n",
    "\n",
    "print(text)\n",
    "print(\"=\"*90)\n",
    "\n",
    "feature_names = tf_vectorizer.get_feature_names_out()\n",
    "print(feature_names)\n",
    "print(\"=\"*90)\n",
    "\n",
    "tf_dense = tf.toarray()\n",
    "print(\"Vectors:\\n\", tf_dense)\n",
    "print(\"=\"*90)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e54cccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get entire text in a list\n",
    "text = list(data['text'])\n",
    "\n",
    "# calling the overwritten Count vectorizer\n",
    "tf_vectorizer = LemmaCountVectorizer(stop_words = \"english\")\n",
    "tf = tf_vectorizer.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7d4693",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)\n",
    "print(\"=\"*90)\n",
    "\n",
    "feature_names = tf_vectorizer.get_feature_names_out()\n",
    "print(feature_names)\n",
    "print(\"=\"*90)\n",
    "\n",
    "tf_dense = tf.toarray()\n",
    "print(\"Vectors:\\n\", tf_dense)\n",
    "print(\"=\"*90)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
