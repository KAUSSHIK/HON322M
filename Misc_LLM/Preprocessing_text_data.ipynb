{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eef4ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b141993",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/kausshik/HON322M/Misc_LLM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b03099",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Hotel_review.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a0d425",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255fb04f",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "1. ***Tokenization*** - The process of breaking down the text into individual words or \"tokens\".\n",
    "\n",
    "2. ***Stopwords Removal*** - Eliminating frequently occurring but less meaningful words (e.g., \"the,\" \"and,\" \"is\") from text data\n",
    "\n",
    "3. ***Stemming and Lemmatization*** - reduce words to their base or root form. \n",
    "\n",
    "    - ***Stemming*** - Simplify words by removing suffixes (e.g., running, runs, and run), but it might result in non-words (e.g., \"leaves\" -> \"leav\"). \n",
    "    \n",
    "    - ***Lemmatization*** - on the other hand, transforms words into their original form while retaining their meaning (e.g., \"leaves\" -> \"leaf\").\n",
    "\n",
    "5. ***Vectorization*** - The conversion of text into a numerical format, such as vectors. A common approach is the bag-of-words model, where a matrix is created to store word frequencies (word counts) for each document or text in the corpus. This process is often referred to as the vectorization of the raw text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdbfb1f",
   "metadata": {},
   "source": [
    "### Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ca3f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cb2ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2deeaacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if only apply split to segregate words\n",
    "first_text = data['text'][0]\n",
    "\n",
    "print(first_text)\n",
    "print(\"=\"*90)\n",
    "print(first_text.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9016a16",
   "metadata": {},
   "source": [
    "\"comma and period\" are not seperated as one term; e.g., \"proposal.\"\n",
    "\n",
    "nltk library \"word_tokenize()\": split singular words as well as puctuations into separate elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628d2544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using nltk.word_tokenize()\n",
    "first_text_list = nltk.word_tokenize(first_text)\n",
    "\n",
    "print(first_text_list)\n",
    "# now it's well seprated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be7d0be",
   "metadata": {},
   "source": [
    "Another popular pre-processing library for text data is `spacy`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b69d41",
   "metadata": {},
   "source": [
    "### Stopword Removal\n",
    "Stop words include terms such as \"to\" or \"the\" and therefore, it would be to our benefit to remove them during the pre-processing phase.\n",
    "\n",
    "NLTK comes with a list of 179 english stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab60142d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc33cfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using nltk.corpus.stopwords.words('english') to remove stop words\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993babe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ce043e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep words that do not contain stopwords\n",
    "first_text_list_cleaned = [word for word in first_text_list if word.lower() not in stopwords]\n",
    "\n",
    "print(first_text_list_cleaned)\n",
    "print(\"=\"*90)\n",
    "print(\"Length of original list: {0} words\\n\"\n",
    "     \"Length of list after stopwords removal: {1} words\"\n",
    "     .format(len(first_text_list), len(first_text_list_cleaned)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c8d788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as [word for word in first_text_list if word.lower() not in stopwords]:\n",
    "first_text_list_cleaned = []\n",
    "\n",
    "for word in first_text_list:\n",
    "    if word.lower() not in stopwords:\n",
    "        first_text_list_cleaned.append(word)\n",
    "        \n",
    "print(first_text_list_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e62449",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization\n",
    "After removal of stopwords, the next stage of NLP that I would like to introduce is the process of Stemming. The work at this stage attempts to reduce as many different variations of similar words into a single term ( different branches all reduced to single word stem). Therefore if we have \"running\", \"runs\" and \"run\", we would really want these three distinct words to collapse into just the word \"run\". (However of course we lose the past, present or future tense)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c17322",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f73ce8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The stemmed form of running is: {}\".format(stemmer.stem(\"running\")))\n",
    "print(\"The stemmed form of runs is : {}\".format(stemmer.stem(\"runs\")))\n",
    "print(\"The stemmed form of run is : {}\".format(stemmer.stem(\"run\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8416c91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The stemmed form of leaves is: {}\".format(stemmer.stem(\"leaves\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf6afd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a686959",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# define lemm, and use lemm.lemmatize() to do lemmatization\n",
    "lemm = WordNetLemmatizer()\n",
    "print(\"The lemmatized form of leaves is: {}\".format(lemm.lemmatize(\"leaves\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8751fc",
   "metadata": {},
   "source": [
    "The lemmatizer is working; making the words much more lexical sense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8092ef",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "\n",
    "A machine can read in bits and numbers and therefore we will first need to convert our text into numbers (Machine learning algorithms operate on a numeric feature space) for which we utilise a very common approach known as the Bag-of-Words.\n",
    "\n",
    "***The Bag of Words approach*** -\n",
    "This approach uses the counts of words and records the occurrence of each word. For example given these two sentences \"I love to eat Burgers\", \"I love to eat Fries\", we first tokenize to obtain our vocabulary of 6 words from which we can get the word counts for - [I, love, to, eat, Burgers, Fries].\n",
    "\n",
    "Each word is a feature and each row is a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dc4302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer(): Convert a collection of text documents to a matrix of token counts\n",
    "# Defining our sentence\n",
    "sentence = [\"I love to eat Burgers\",\n",
    "           \"I love to eat Fries\"]\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words= \"english\") \n",
    "\n",
    "# fit\n",
    "sentence_transform = vectorizer.fit_transform(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3cac96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The features are: \\n {}\".format(vectorizer.get_feature_names_out())) # \\n: add new line\n",
    "print(\"The vectorized array looks like: \\n {}\".format(sentence_transform.toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21556634",
   "metadata": {},
   "source": [
    "First row: 1 burger, 1 eat, 0 fries, 1 love\n",
    "\n",
    "Second row: 0 burger, 1 eat, 1 fries, 1 love"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce141a5",
   "metadata": {},
   "source": [
    "### Putting all the preprocessing steps together\n",
    "Do not need to go through all the steps in tokenization, stopword removals, stemming/lemmatizing, and vectorization.\n",
    "\n",
    "Sklearn's tokenizer discards all single character terms like ('a', 'w' etc) and also lower cases all terms by default. Filtering out stopwords in Sklearn is as convenient as passing the value 'english' into the argument \"stop_words\" where a built-in English stopword list is automatically used.\n",
    "\n",
    "Unfortunately, there is no built-in lemmatizer in the vectorizer while we can extend the CountVectorizer class by overwriting the \"build_analyzer\" method as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6591fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# define lemm to do lemmatization\n",
    "lemm = WordNetLemmatizer()\n",
    "\n",
    "# define a class to extend the CountVectorizer class with a lemmatizer\n",
    "class LemmaCountVectorizer(CountVectorizer):\n",
    "\n",
    "    # build_analyzer(): Return a callable to process input data. \n",
    "    # The callable handles that handles preprocessing, tokenization, and n-grams generation.\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(LemmaCountVectorizer, self).build_analyzer()   # self=countvectorizer\n",
    "        return lambda doc: (lemm.lemmatize(w) for w in analyzer(doc))\n",
    "        \n",
    "#  super() method lets you access methods in a parent class. You can think of super() as a way to jump up to \n",
    "# view the methods in the class from which another class is inherited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de76474",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"I love to eat Burgers\", \"I love to eat Fries\"]\n",
    "\n",
    "tf_vectorizer = LemmaCountVectorizer(stop_words = \"english\")\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(text)\n",
    "\n",
    "print(text)\n",
    "print(\"=\"*90)\n",
    "\n",
    "feature_names = tf_vectorizer.get_feature_names_out()\n",
    "print(feature_names)\n",
    "print(\"=\"*90)\n",
    "\n",
    "tf_dense = tf.toarray()\n",
    "print(\"Vectors:\\n\", tf_dense)\n",
    "print(\"=\"*90)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e54cccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get entire text in a list\n",
    "text = list(data['text'])\n",
    "\n",
    "# calling the overwritten Count vectorizer\n",
    "tf_vectorizer = LemmaCountVectorizer(stop_words = \"english\")\n",
    "tf = tf_vectorizer.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7d4693",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)\n",
    "print(\"=\"*90)\n",
    "\n",
    "feature_names = tf_vectorizer.get_feature_names_out()\n",
    "print(feature_names)\n",
    "print(\"=\"*90)\n",
    "\n",
    "tf_dense = tf.toarray()\n",
    "print(\"Vectors:\\n\", tf_dense)\n",
    "print(\"=\"*90)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
